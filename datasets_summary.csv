name,url,citation,capture_settings,data_size,advantages,limitations
Animal Kingdom (CVPR2022),https://github.com/sutdcv/Animal-Kingdom,Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding (CVPR 2022). https://openaccess.thecvf.com/content/CVPR2022/papers/Ng_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_CVPR_2022_paper.pdf,"Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Discussions Collaborate outside of code Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support To see all available qualifiers, see our documentation . Video Grounding Collaborative Action Recognition (CARe) Model",50 hours; 850 species,"[CVPR2022] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding This is the official repository for [CVPR2022] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding Xun Long NG, Kian Eng ONG, Qichen ZHENG, Yun NI, Si Yong YEO, Jun LIU Information Systems Technology and Design, Singapore University of Technology and Design, Singapore @InProceedings {
    Ng_2022_CVPR, author = { Ng, Xun Long and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si Yong and Liu, Jun } , title = { Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding } , booktitle = { Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) } , month = { June } , year = { 2022 } , pages = { 19023-19034 } }","Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Issues Plan and track work Search code, repositories, users, issues, pull requests..."
APT-36K (Animal Pose Tracking),https://github.com/pandorgan/APT-36K,APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking. (NeurIPS / OpenReview / paper).,"Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Discussions Collaborate outside of code Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support To see all available qualifiers, see our documentation . The goal of APT-36K is to provide a large-scale benchmark for animal pose estimation and tracking in real-world scenarios, which has been rarely explored in prior art. To this end, we resort to real-world video websites, i.e., YouTube, and carefully collect and filter 2,400 video clips covering 30 different animal species from different scenes, e.g., zoo, forest, and desert. Then we manually set the frame sampling rate for each video to ensure there are noticeable movement and posture differences for each animal in the sub-sampled video clips. Specifically, each clip contains 15 frames after the sampling process.The whole data collection, cleaning, annotation, and check process takes about 2,000 person-hours. A total of 36,000 images are finally labeled, following the COCO labeling format. There are typically 17 keypoints labeled for each animal instance, including two eyes, one nose, one neck, one tail, two shoulders, two elbows, two knees, two hips, and four paws. We also calculate the distributions of the keypoint motion, IOU between tracked bounding boxes in adjacent frames, and the aspect ratio of the annotated bounding boxes in our APT-36K dataset. As shown in (a), the motion distribution and average motion distance vary a lot for different keypoints, e.g., the average motion distance of paws is over 50 pixels, which is much larger than that of eyes or necks (about 35 pixels). Moreover, the motion magnitudes of shoulder, knee, and hips lie between those of eyes and paws, which is in line with the movement characteristics of four-leg animals. Besides, most of the instances have small IOU scores between their tracked bounding boxes in adjacent frames, implying large motion is very common in APT-36K, as demonstrated in (b). It can also be observed from (c) that the aspect ratio of the bounding box varies a lot from less than 0.4 to more than 3.1. It is because APT-36K contains diverse animals with different actions, e.g., running rabbits and climbing monkeys. These results illustrate the diversity of APT-36K.","2,400 video clips; 15 frames; 36,000 frames; 36,000 images","APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. To fill this gap, we make the first step and propose APT-36K , i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a valuable animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. Annotated files and corresponding images our datasets can be downloaded at https://1drv.ms/u/s!AimBgYV7JjTlgcZ9zLyl5KnM3dKMgg?e=uaaLz5 . The individual annotation files can be downloaded at https://1drv.ms/u/s!AimBgYV7JjTlgTuYdjjtYON3sxEZ?e=5deTDn . The goal of APT-36K is to provide a large-scale benchmark for animal pose estimation and tracking in real-world scenarios, which has been rarely explored in prior art. To this end, we resort to real-world video websites, i.e., YouTube, and carefully collect and filter 2,400 video clips covering 30 different animal species from different scenes, e.g., zoo, forest, and desert. Then we manually set the frame sampling rate for each video to ensure there are noticeable movement and posture differences for each animal in the sub-sampled video clips. Specifically, each clip contains 15 frames after the sampling process.The whole data collection, cleaning, annotation, and check process takes about 2,000 person-hours. A total of 36,000 images are finally labeled, following the COCO labeling format. There are typically 17 keypoints labeled for each animal instance, including two eyes, one nose, one neck, one tail, two shoulders, two elbows, two knees, two hips, and four paws.","Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Issues Plan and track work Search code, repositories, users, issues, pull requests..."
iNaturalist (subset: ba188/iNaturalist_v2),https://huggingface.co/datasets/ba188/iNaturalist_v2,HuggingFace - ba188/iNaturalist_v2 dataset page.,"For each of the 1,079 observations included in this dataset, there is information about the quality of the associated image (quality_grade), a species label (species_guess), *Note: the embeddings were created using the ResNet50 model with imagenet weight, the top layer removed, and average pooling. They are in the dataset as strings of varying length Example use: This dataset may be used for training and testing image classification models as in this example ( https://colab.research.google.com/drive/19yogkKM7Sxuk92LuC5kkNiZGOWeaH2TH?usp=sharing ). 'observations_count': integer value describing how many times that species has been recorded on iNaturalist including embeddings of the photos that can be used for training or testing image classification models. Additionally, iNaturalist jitters geographical coordinates of specific species for various reasons (e.g. they are protected/endangered) before making data available to viewers.",,,
MammalNet,https://mammal-net.github.io/,MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding (CVPR 2023).,MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding MammalNet Video Dataset,,MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding,
