[
  {
    "name": "Animal Kingdom (CVPR2022)",
    "url": "https://github.com/sutdcv/Animal-Kingdom",
    "citation": "Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding (CVPR 2022). https://openaccess.thecvf.com/content/CVPR2022/papers/Ng_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_CVPR_2022_paper.pdf",
    "raw_text_snippet": "Navigation Menu\n\nPlatform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features\n\nGitHub Copilot Write better code with AI\n\nGitHub Spark New Build and deploy intelligent apps\n\nGitHub Models New Manage and compare prompts\n\nGitHub Advanced Security Find and fix vulnerabilities\n\nActions Automate any workflow\n\nCodespaces Instant dev environments\n\nIssues Plan and track work\n\nCode Review Manage code changes\n\nDiscussions Collaborate outside of code\n\nCode Search Find more, search less\n\nWhy GitHub\n\nDocumentation\n\nGitHub Skills\n\nBlog\n\nGitHub Marketplace\n\nMCP Registry\n\nSolutions By company size Enterprises Small and medium teams Startups Nonprofits By use case App Modernization DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions\n\nEnterprises\n\nSmall and medium teams\n\nStartups\n\nNonprofits\n\nApp Modernization\n\nDevSecOps\n\nDevOps\n\nCI/CD\n\nView all use cases\n\nHealthcare\n\nFinancial services\n\nManufacturing\n\nGovernment\n\nView all industries\n\nResources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events & Webinars Ebooks & Whitepapers Customer Stories Partners Executive Insights\n\nAI\n\nDevOps\n\nSecurity\n\nSoftware Development\n\nView all\n\nLearning Pathways\n\nEvents & Webinars\n\nEbooks & Whitepapers\n\nCustomer Stories\n\nPartners\n\nExecutive Insights\n\nOpen Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections\n\nGitHub Sponsors Fund open source developers\n\nThe ReadME Project GitHub community articles\n\nTopics\n\nTrending\n\nCollections\n\nEnterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support\n\nEnterprise platform AI-powered developer platform\n\nGitHub Advanced Security Enterprise-grade security features\n\nCopilot for business Enterprise-grade AI features\n\nPremium Support Enterprise-grade 24/7 support\n\nPricing\n\nSearch code, repositories, users, issues, pull requests...\n\nProvide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\nSaved searches\n\nUse saved searches to filter your results more quickly\n\nTo see all available qualifiers, see our documentation .\n\nNotifications You must be signed in to change notification settings\n\nFork 12\n\nStar 150\n\n[CVPR2022] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding\n\nCode\n\nPull requests 0\n\nActions\n\nProjects 0\n\nSecurity Uh oh! There was an error while loading. Please reload this page .\n\nUh oh!\n\nThere was an error while loading. Please reload this page .\n\nThere was an error while loading. Please reload this page .\n\nInsights\n\nCode\n\nPull requests\n\nActions\n\nProjects\n\nSecurity\n\nInsights\n\nsutdcv/Animal-Kingdom\n\nFolders and files\n\nLatest commit\n\nHistory\n\nRepository files navigation\n\nREADME\n\nAnimal Kingdom Dataset\n\nThis is the official repository for [CVPR2022] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding Xun Long NG, Kian Eng ONG, Qichen ZHENG, Yun NI, Si Yong YEO, Jun LIU Information Systems Technology and Design, Singapore University of Technology and Design, Singapore\n\nDataset and Codes\n\nDownload dataset and codes here\n\nNOTE: The codes of the models for all tasks have been released. Codes are included in the folder of the dataset. After you download our dataset, you can find the corresponding codes for each task. Helper scrip...",
    "capture_settings": "Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Discussions Collaborate outside of code Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support To see all available qualifiers, see our documentation . Video Grounding Collaborative Action Recognition (CARe) Model",
    "data_size": "50 hours; 850 species",
    "advantages": "[CVPR2022] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding This is the official repository for [CVPR2022] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding Xun Long NG, Kian Eng ONG, Qichen ZHENG, Yun NI, Si Yong YEO, Jun LIU Information Systems Technology and Design, Singapore University of Technology and Design, Singapore @InProceedings {\n    Ng_2022_CVPR, author = { Ng, Xun Long and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si Yong and Liu, Jun } , title = { Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding } , booktitle = { Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) } , month = { June } , year = { 2022 } , pages = { 19023-19034 } }",
    "limitations": "Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Issues Plan and track work Search code, repositories, users, issues, pull requests..."
  },
  {
    "name": "APT-36K (Animal Pose Tracking)",
    "url": "https://github.com/pandorgan/APT-36K",
    "citation": "APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking. (NeurIPS / OpenReview / paper).",
    "raw_text_snippet": "Navigation Menu\n\nPlatform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features\n\nGitHub Copilot Write better code with AI\n\nGitHub Spark New Build and deploy intelligent apps\n\nGitHub Models New Manage and compare prompts\n\nGitHub Advanced Security Find and fix vulnerabilities\n\nActions Automate any workflow\n\nCodespaces Instant dev environments\n\nIssues Plan and track work\n\nCode Review Manage code changes\n\nDiscussions Collaborate outside of code\n\nCode Search Find more, search less\n\nWhy GitHub\n\nDocumentation\n\nGitHub Skills\n\nBlog\n\nGitHub Marketplace\n\nMCP Registry\n\nSolutions By company size Enterprises Small and medium teams Startups Nonprofits By use case App Modernization DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions\n\nEnterprises\n\nSmall and medium teams\n\nStartups\n\nNonprofits\n\nApp Modernization\n\nDevSecOps\n\nDevOps\n\nCI/CD\n\nView all use cases\n\nHealthcare\n\nFinancial services\n\nManufacturing\n\nGovernment\n\nView all industries\n\nResources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events & Webinars Ebooks & Whitepapers Customer Stories Partners Executive Insights\n\nAI\n\nDevOps\n\nSecurity\n\nSoftware Development\n\nView all\n\nLearning Pathways\n\nEvents & Webinars\n\nEbooks & Whitepapers\n\nCustomer Stories\n\nPartners\n\nExecutive Insights\n\nOpen Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections\n\nGitHub Sponsors Fund open source developers\n\nThe ReadME Project GitHub community articles\n\nTopics\n\nTrending\n\nCollections\n\nEnterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support\n\nEnterprise platform AI-powered developer platform\n\nGitHub Advanced Security Enterprise-grade security features\n\nCopilot for business Enterprise-grade AI features\n\nPremium Support Enterprise-grade 24/7 support\n\nPricing\n\nSearch code, repositories, users, issues, pull requests...\n\nProvide feedback\n\nWe read every piece of feedback, and take your input very seriously.\n\nSaved searches\n\nUse saved searches to filter your results more quickly\n\nTo see all available qualifiers, see our documentation .\n\nNotifications You must be signed in to change notification settings\n\nFork 1\n\nStar 51\n\nThe extension of this dataset (APTv2) can be found at:\n\nCode\n\nIssues 5\n\nPull requests 0\n\nActions\n\nProjects 0\n\nSecurity Uh oh! There was an error while loading. Please reload this page .\n\nUh oh!\n\nThere was an error while loading. Please reload this page .\n\nThere was an error while loading. Please reload this page .\n\nInsights\n\nCode\n\nIssues\n\nPull requests\n\nActions\n\nProjects\n\nSecurity\n\nInsights\n\npandorgan/APT-36K\n\nFolders and files\n\nLatest commit\n\nHistory\n\nRepository files navigation\n\nREADME\n\nAPT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking\n\nIntroduction | APT-36K | Demo | Statement\n\nIntroduction\n\nAnimal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. To fill this gap, we make the first step and propose APT-36K , i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips colle...",
    "capture_settings": "Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Discussions Collaborate outside of code Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support To see all available qualifiers, see our documentation . The goal of APT-36K is to provide a large-scale benchmark for animal pose estimation and tracking in real-world scenarios, which has been rarely explored in prior art. To this end, we resort to real-world video websites, i.e., YouTube, and carefully collect and filter 2,400 video clips covering 30 different animal species from different scenes, e.g., zoo, forest, and desert. Then we manually set the frame sampling rate for each video to ensure there are noticeable movement and posture differences for each animal in the sub-sampled video clips. Specifically, each clip contains 15 frames after the sampling process.The whole data collection, cleaning, annotation, and check process takes about 2,000 person-hours. A total of 36,000 images are finally labeled, following the COCO labeling format. There are typically 17 keypoints labeled for each animal instance, including two eyes, one nose, one neck, one tail, two shoulders, two elbows, two knees, two hips, and four paws. We also calculate the distributions of the keypoint motion, IOU between tracked bounding boxes in adjacent frames, and the aspect ratio of the annotated bounding boxes in our APT-36K dataset. As shown in (a), the motion distribution and average motion distance vary a lot for different keypoints, e.g., the average motion distance of paws is over 50 pixels, which is much larger than that of eyes or necks (about 35 pixels). Moreover, the motion magnitudes of shoulder, knee, and hips lie between those of eyes and paws, which is in line with the movement characteristics of four-leg animals. Besides, most of the instances have small IOU scores between their tracked bounding boxes in adjacent frames, implying large motion is very common in APT-36K, as demonstrated in (b). It can also be observed from (c) that the aspect ratio of the bounding box varies a lot from less than 0.4 to more than 3.1. It is because APT-36K contains diverse animals with different actions, e.g., running rabbits and climbing monkeys. These results illustrate the diversity of APT-36K.",
    "data_size": "2,400 video clips; 15 frames; 36,000 frames; 36,000 images",
    "advantages": "APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. To fill this gap, we make the first step and propose APT-36K , i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a valuable animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. Annotated files and corresponding images our datasets can be downloaded at https://1drv.ms/u/s!AimBgYV7JjTlgcZ9zLyl5KnM3dKMgg?e=uaaLz5 . The individual annotation files can be downloaded at https://1drv.ms/u/s!AimBgYV7JjTlgTuYdjjtYON3sxEZ?e=5deTDn . The goal of APT-36K is to provide a large-scale benchmark for animal pose estimation and tracking in real-world scenarios, which has been rarely explored in prior art. To this end, we resort to real-world video websites, i.e., YouTube, and carefully collect and filter 2,400 video clips covering 30 different animal species from different scenes, e.g., zoo, forest, and desert. Then we manually set the frame sampling rate for each video to ensure there are noticeable movement and posture differences for each animal in the sub-sampled video clips. Specifically, each clip contains 15 frames after the sampling process.The whole data collection, cleaning, annotation, and check process takes about 2,000 person-hours. A total of 36,000 images are finally labeled, following the COCO labeling format. There are typically 17 keypoints labeled for each animal instance, including two eyes, one nose, one neck, one tail, two shoulders, two elbows, two knees, two hips, and four paws.",
    "limitations": "Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Issues Plan and track work Search code, repositories, users, issues, pull requests..."
  },
  {
    "name": "iNaturalist (subset: ba188/iNaturalist_v2)",
    "url": "https://huggingface.co/datasets/ba188/iNaturalist_v2",
    "citation": "HuggingFace - ba188/iNaturalist_v2 dataset page.",
    "raw_text_snippet": "Models\n\nDatasets\n\nSpaces\n\nCommunity\n\nDocs\n\nEnterprise\n\nPricing\n\nLog In\n\nSign Up\n\nDatasets: ba188 / iNaturalist_v2 like 1\n\nPrevious\n\n1\n\n2\n\n3\n\n...\n\n9\n\nNext\n\nDataset Details Dataset Description Dataset Sources\n\nDataset Description\n\nDataset Sources\n\nUses Direct Use\n\nDirect Use\n\nDataset Structure\n\nDataset Creation Curation Rationale\n\nCuration Rationale\n\nDataset Card for Dataset Name\n\nThis dataset is comprised of 1,079 observations that were posted on the iNaturalist app. iNaturalist is a website and mobile app that 'aims to \nprovide a crowd-sourced identification system' for plants, insects, and animals.\n\nDataset Details\n\nDataset Description\n\nFor each of the 1,079 observations included in this dataset, there is information about the quality of the associated image (quality_grade), a species label (species_guess), \na date of observation (observed_on), the location it was observed (both latitude/longitude coordinates and a place name, location and place_name, respectively), a name that\nusually matches the species guess but sometimes provides other information such as the genus (name), the number of times that species has been observed before on the app (observations_count), \na higher-order taxonimic classification (taxon), a common name for the species/genus/class (common_name), a link to a relevant wikipedia article (wikipedia_link), a url to a photo from that \nobservation (photo), and an embedding (embedding). \\\n\n*Note: the embeddings were created using the ResNet50 model with imagenet weight, the top layer removed, and average pooling. They are in the dataset as strings of varying length\nand should be converted to arrays of shape (2048,) by splitting at ',' before use for modelling.\n\nDataset Sources\n\nAPI: Data originally accessed via the iNaturalist API at https://api.inaturalist.org\n\nUses\n\nDirect Use\n\nExample use: This dataset may be used for training and testing image classification models as in this example ( https://colab.research.google.com/drive/19yogkKM7Sxuk92LuC5kkNiZGOWeaH2TH?usp=sharing ).\n\nDataset Structure\n\nColumns:\n\n'quality_grade': string with possible values 'needs_id', 'research', or 'casual'; describes the quality of the photograph associated with the observation\n\n'species_guess': string that describes the species (or sometimes a higher order taxa) of the observation\n\n'observed_on': string that contains the date of the observation (yyyy-mm-dd)\n\n'location': string that contains the geographical coordinates of where the observation was made (latitude, longitude)\n\n'place_name': string that describes the location where the observation was made (not in standard format)\n\n'name': string containing some higher order taxonomic classification of the observation (e.g. family, genus)\n\n'observations_count': integer value describing how many times that species has been recorded on iNaturalist\n\n'taxon': string containg the kingdom or class of the observation\n\n'common_name': string containing a common name for the observation (sometimes the species, sometimes a higher order taxon; only included in 70% of the data)\n\n'wikipedia_link': string url to a relevant wikipedia article\n\n'photo': string url to a photograph associated with the observation\n\n'embedding': strings of varying lengths created from ResNet50 embeddings (should be converted into arrays of shape (2048,) before use)\n\nThe dataset has a 80/20 training/testing split (training set contains 863 rows, testing set contains 216 rows) created using the train_test_split from sklearn.\n\nDataset Creation\n\nThis dataset was created from data pulled from the iNatualist API (linked above). 9,990 requests were made to the API, then filtered to only include those observations that \nhad a photograph and a species guess in the Latin alphabet. There were 1,079 remaining observations, all of which are included in this dataset. \\\n\nAll of the information aside from the embeddings was pulled directly from the API. The process of creating the embedding column is described above (see: Dataset Desc...",
    "capture_settings": "For each of the 1,079 observations included in this dataset, there is information about the quality of the associated image (quality_grade), a species label (species_guess), *Note: the embeddings were created using the ResNet50 model with imagenet weight, the top layer removed, and average pooling. They are in the dataset as strings of varying length Example use: This dataset may be used for training and testing image classification models as in this example ( https://colab.research.google.com/drive/19yogkKM7Sxuk92LuC5kkNiZGOWeaH2TH?usp=sharing ). 'observations_count': integer value describing how many times that species has been recorded on iNaturalist including embeddings of the photos that can be used for training or testing image classification models. Additionally, iNaturalist jitters geographical coordinates of specific species for various reasons (e.g. they are protected/endangered) before making data available to viewers.",
    "data_size": "",
    "advantages": "",
    "limitations": ""
  },
  {
    "name": "MammalNet",
    "url": "https://mammal-net.github.io/",
    "citation": "MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding (CVPR 2023).",
    "raw_text_snippet": "Home\n\nDataset\n\nOrganizers\n\nContact\n\nMammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding\n\nDataset\n\nMammalNet Video Dataset\n\nOrganizers\n\nDataset Organizers\n\nJun Chen\n\nMing Hu\n\nDarren J. Cooker\n\nMichael L. Berumen\n\nBlair Costelloe\n\nSara Beery\n\nAnna Rohrbach\n\nMohamed Elhoseiny...",
    "capture_settings": "MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding MammalNet Video Dataset",
    "data_size": "",
    "advantages": "MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding",
    "limitations": ""
  }
]